{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network From scratch python.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inesch2/ASD-autism-detection/blob/main/Neural_Network_From_scratch_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLbW0_sMdzVV"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# sigmoid\n",
        "\n",
        "def sigmoid(z):\n",
        "\n",
        "  return( 1/(1+np.exp(-z)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJK95QRQewf3"
      },
      "source": [
        "def initialize_parameters(n_x,n_h,n_y):\n",
        "\n",
        "  W1 = np.random.randn(n_h,n_x)\n",
        "  b1 = np.zeros((n_h,1))\n",
        "  W2 = np.random.randn(n_y,n_h)\n",
        "  b2 = np.zeros((n_y,1))\n",
        "\n",
        "  parameters = {\n",
        "      \"W1\":W1,\n",
        "      \"b1\":b1,\n",
        "      \"W2\":W2,\n",
        "      \"b2\":b2\n",
        "  }\n",
        "  return(parameters)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyV3_EGTraWZ"
      },
      "source": [
        "def Relu(x):\n",
        "  return(max(0.0,x.any()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0wOBXJ8gbOD"
      },
      "source": [
        "def forward_prop(X,parameters):\n",
        "\n",
        "  W1 = parameters[\"W1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "\n",
        "\n",
        "  Z1 = np.dot(W1,X) + b1\n",
        "  A1 = np.tanh(Z1)\n",
        "  Z2 = np.dot(W2,A1) + b2\n",
        "  A2 = sigmoid(Z2)\n",
        "\n",
        "  cache = {\n",
        "      \"A1\":A1,\n",
        "      \"A2\": A2\n",
        "  }\n",
        "\n",
        "  return(A2,cache)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ65Y1VdsRoF"
      },
      "source": [
        "def backward_prop(X,Y,cache,parameters):\n",
        "  \n",
        "\n",
        "  A1 = cache[\"A1\"]\n",
        "  A2 = cache[\"A2\"]\n",
        "\n",
        "  W2 = parameters[\"W2\"]\n",
        "\n",
        "  dZ2 = A2 - Y \n",
        "  dW2 = np.dot(dZ2,A1.T)/m\n",
        "  db2 = np.sum(dZ2,axis=1,keepdims=True)/m\n",
        "\n",
        "  dZ1 = np.multiply(np.dot(W2.T,dZ2),1-np.power(A1,2))\n",
        "  dW1 = np.dot(dZ1,X.T) /m\n",
        "  db1 = np.sum(dZ1,axis=1,keepdims=True)\n",
        "\n",
        "  grads = {\n",
        "      \"dW1\": dW1,\n",
        "      \"db1\":db1,\n",
        "      \"dW2\": dW2,\n",
        "      \"db2\": db2\n",
        "  }\n",
        "  return(grads)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO0sZ2MD34fo"
      },
      "source": [
        "def update_parameters(parameters,grads,learning_rate):\n",
        "\n",
        "  W1 = parameters[\"W1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "\n",
        "  dW1 = grads[\"dW1\"]\n",
        "  dW2 = grads[\"dW2\"]\n",
        "  db1 = grads[\"db1\"]\n",
        "  db2 = grads[\"db2\"]\n",
        "\n",
        "  # update weights and biases gradient descent\n",
        "  W1 = W1 - learning_rate*dW1\n",
        "  W2 = W2 - learning_rate*dW2\n",
        "  b1 = b1 - learning_rate*db1\n",
        "  b2 = b2 - learning_rate*db2\n",
        "\n",
        "  \n",
        "  new_parameters = {\n",
        "    \"W1\": W1,\n",
        "    \"W2\": W2,\n",
        "    \"b1\" : b1,\n",
        "    \"b2\" : b2\n",
        "  }\n",
        "  return(new_parameters)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "844xmQVy7lPY"
      },
      "source": [
        "def calculate_cost(A2,Y):\n",
        "\n",
        "  cost = -np.sum(np.multiply(Y,np.log(A2))+ np.multiply(1-Y,np.log(1-A2)))/m \n",
        "  cost = np.squeeze(cost)\n",
        "  return(cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrI1TtVB6lVK"
      },
      "source": [
        "  def model(X,Y,n_x,n_h,n_y,num_of_iters,learning_rate):\n",
        "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
        "\n",
        "    for i in range(0,num_of_iters+1):\n",
        "      A2,cache = forward_prop(X,parameters)\n",
        "\n",
        "      cost = calculate_cost(A2,Y)\n",
        "\n",
        "      grads = backward_prop(X,Y,cache,parameters)\n",
        "\n",
        "      parameters = update_parameters(parameters,grads,learning_rate)\n",
        "\n",
        "      if (i%100 ==0):\n",
        "        print(\"Cost after iteration# {:d}:{:f}\".format(i,cost))\n",
        "\n",
        "    return(parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5OfuQ0V99tx"
      },
      "source": [
        "def predict(X, parameters):\n",
        "  \n",
        "  a2, cache = forward_prop(X, parameters)\n",
        "  yhat = a2\n",
        "  yhat = np.squeeze(yhat)\n",
        "\n",
        "  if(yhat >= 0.5):\n",
        "    y_predict = 1\n",
        "  else:\n",
        "    y_predict = 0\n",
        "\n",
        "  return(y_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omiRgWBv-Sbs",
        "outputId": "f5e3880a-dd25-4d07-9edb-d60ce8fef947"
      },
      "source": [
        "np.random.seed(2)\n",
        "\n",
        "# The 4 training examples by columns\n",
        "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
        "\n",
        "# The outputs of the XOR for every example in X\n",
        "Y = np.array([[0, 1, 1, 0]])\n",
        "\n",
        "\n",
        "# No. of training examples\n",
        "m = X.shape[1]\n",
        "\n",
        "# Set the hyperparameters\n",
        "n_x = 2     #No. of neurons in first layer\n",
        "n_h = 2     #No. of neurons in hidden layer\n",
        "n_y = 1     #No. of neurons in output layer\n",
        "\n",
        "num_of_iters = 10000\n",
        "\n",
        "learning_rate = 0.3\n",
        "\n",
        "trained_parameters = model(X, Y, n_x, n_h, n_y, num_of_iters, learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration# 0:0.856267\n",
            "Cost after iteration# 100:0.320178\n",
            "Cost after iteration# 200:0.092824\n",
            "Cost after iteration# 300:0.050814\n",
            "Cost after iteration# 400:0.034637\n",
            "Cost after iteration# 500:0.026169\n",
            "Cost after iteration# 600:0.020985\n",
            "Cost after iteration# 700:0.017496\n",
            "Cost after iteration# 800:0.014990\n",
            "Cost after iteration# 900:0.013106\n",
            "Cost after iteration# 1000:0.011639\n",
            "Cost after iteration# 1100:0.010464\n",
            "Cost after iteration# 1200:0.009503\n",
            "Cost after iteration# 1300:0.008702\n",
            "Cost after iteration# 1400:0.008025\n",
            "Cost after iteration# 1500:0.007445\n",
            "Cost after iteration# 1600:0.006942\n",
            "Cost after iteration# 1700:0.006503\n",
            "Cost after iteration# 1800:0.006115\n",
            "Cost after iteration# 1900:0.005771\n",
            "Cost after iteration# 2000:0.005464\n",
            "Cost after iteration# 2100:0.005187\n",
            "Cost after iteration# 2200:0.004937\n",
            "Cost after iteration# 2300:0.004709\n",
            "Cost after iteration# 2400:0.004502\n",
            "Cost after iteration# 2500:0.004312\n",
            "Cost after iteration# 2600:0.004137\n",
            "Cost after iteration# 2700:0.003976\n",
            "Cost after iteration# 2800:0.003827\n",
            "Cost after iteration# 2900:0.003689\n",
            "Cost after iteration# 3000:0.003560\n",
            "Cost after iteration# 3100:0.003440\n",
            "Cost after iteration# 3200:0.003327\n",
            "Cost after iteration# 3300:0.003222\n",
            "Cost after iteration# 3400:0.003123\n",
            "Cost after iteration# 3500:0.003030\n",
            "Cost after iteration# 3600:0.002943\n",
            "Cost after iteration# 3700:0.002860\n",
            "Cost after iteration# 3800:0.002782\n",
            "Cost after iteration# 3900:0.002708\n",
            "Cost after iteration# 4000:0.002637\n",
            "Cost after iteration# 4100:0.002571\n",
            "Cost after iteration# 4200:0.002507\n",
            "Cost after iteration# 4300:0.002447\n",
            "Cost after iteration# 4400:0.002389\n",
            "Cost after iteration# 4500:0.002334\n",
            "Cost after iteration# 4600:0.002282\n",
            "Cost after iteration# 4700:0.002232\n",
            "Cost after iteration# 4800:0.002184\n",
            "Cost after iteration# 4900:0.002138\n",
            "Cost after iteration# 5000:0.002094\n",
            "Cost after iteration# 5100:0.002051\n",
            "Cost after iteration# 5200:0.002011\n",
            "Cost after iteration# 5300:0.001972\n",
            "Cost after iteration# 5400:0.001934\n",
            "Cost after iteration# 5500:0.001898\n",
            "Cost after iteration# 5600:0.001863\n",
            "Cost after iteration# 5700:0.001829\n",
            "Cost after iteration# 5800:0.001797\n",
            "Cost after iteration# 5900:0.001766\n",
            "Cost after iteration# 6000:0.001735\n",
            "Cost after iteration# 6100:0.001706\n",
            "Cost after iteration# 6200:0.001678\n",
            "Cost after iteration# 6300:0.001651\n",
            "Cost after iteration# 6400:0.001624\n",
            "Cost after iteration# 6500:0.001598\n",
            "Cost after iteration# 6600:0.001574\n",
            "Cost after iteration# 6700:0.001550\n",
            "Cost after iteration# 6800:0.001526\n",
            "Cost after iteration# 6900:0.001504\n",
            "Cost after iteration# 7000:0.001482\n",
            "Cost after iteration# 7100:0.001460\n",
            "Cost after iteration# 7200:0.001439\n",
            "Cost after iteration# 7300:0.001419\n",
            "Cost after iteration# 7400:0.001400\n",
            "Cost after iteration# 7500:0.001381\n",
            "Cost after iteration# 7600:0.001362\n",
            "Cost after iteration# 7700:0.001344\n",
            "Cost after iteration# 7800:0.001326\n",
            "Cost after iteration# 7900:0.001309\n",
            "Cost after iteration# 8000:0.001292\n",
            "Cost after iteration# 8100:0.001276\n",
            "Cost after iteration# 8200:0.001260\n",
            "Cost after iteration# 8300:0.001245\n",
            "Cost after iteration# 8400:0.001230\n",
            "Cost after iteration# 8500:0.001215\n",
            "Cost after iteration# 8600:0.001200\n",
            "Cost after iteration# 8700:0.001186\n",
            "Cost after iteration# 8800:0.001172\n",
            "Cost after iteration# 8900:0.001159\n",
            "Cost after iteration# 9000:0.001146\n",
            "Cost after iteration# 9100:0.001133\n",
            "Cost after iteration# 9200:0.001121\n",
            "Cost after iteration# 9300:0.001108\n",
            "Cost after iteration# 9400:0.001096\n",
            "Cost after iteration# 9500:0.001084\n",
            "Cost after iteration# 9600:0.001073\n",
            "Cost after iteration# 9700:0.001062\n",
            "Cost after iteration# 9800:0.001051\n",
            "Cost after iteration# 9900:0.001040\n",
            "Cost after iteration# 10000:0.001029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R5JuuYy_Mau"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}